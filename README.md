# multi-arms-bandit
テーマ：強化学習の多腕バンディット  

参考）書籍：ITエンジニアのための強化学習理論入門(https://gihyo.jp/book/2020/978-4-297-11515-9)   
　　　コード：https://github.com/enakai00/colab_rlbook  

参考書籍、コードを参考に、自分で1から多腕バンディットのコーディングを行った  

各種機能を関数化すると、理解が追い付かないので、一切の関数化を行っていない  
お見苦しい点ご了承ください。  

```python
# スロットの台数
slot_arms = 5

# 各スロットの期待値
e_value = np.random.normal(loc=0.0, scale=1.0, size=slot_arms)
for i in range(slot_arms):
    print("slot No.{0} 期待値: {1}".format(i, e_value[i]))

print()
print("最も期待値の大きい slot は No.{}".format(np.argmax(e_value)))
```

ランダム出力なので、必ず下記の期待値に設定されるわけではないが、一番下の写真は下記の条件での出力である。

slot No.0 期待値: 0.39214341712590317
slot No.1 期待値: -0.20641977501211484
slot No.2 期待値: 0.6860500584941849
slot No.3 期待値: 0.4747609704743502
slot No.4 期待値: -0.7014833706044542

最も期待値の大きい slot は No.2

```python
# 重み
w = 0.1

# ε-greedy スロットの中からランダムで選ぶ(=探索)確率
epsilon = 0.1
```

### 初期値 0.0 の場合  

50回目くらいまでは3番目に期待値の高い No.0 が選択され続けるが、50～300回目くらいまでは最も初期値の高い No.2 の成果が大きくなった。  

探索でランダムに抽出した結果、No.2を選択し、その後も選ばれ続けている結果と思われる。  

また300～340回目くらいにNo.0 が選択され続けているのも、探索でランダムに抽出した結果その後も選ばれ続けたためと思われる。  

![ダウンロード](https://user-images.githubusercontent.com/45703844/88351114-4253e300-cd90-11ea-8458-94895e4b9f71.png)

### 初期値 3.0 の場合  

期待値 0.0との違いは、序盤の130回目くらいまで、すべてのパターンが選択され続けている点である。  

```python
qs2 = [i_value] * slot_arms  
qs2[arm] += w * (reward - qs2[arm])
```

初期値を3.0にした場合は、reward - qs2[arm] がマイナスとなる可能性が高く、初期値0.0の場合は、プラスとなる可能性が高い。  

そのため、初期値3.0の場合は、すべての台を選択しやすくなる。  

つまり、序盤にたまたま選択した台が、実は期待値は最大ではないものの、とりあえず収支がプラスとなっているので選択され続けてしまう、ということを防ぎやすくなる。  

![ダウンロード (1)](https://user-images.githubusercontent.com/45703844/88351118-43851000-cd90-11ea-8c6a-2c222611a2f8.png)
